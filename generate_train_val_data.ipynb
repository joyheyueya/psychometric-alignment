{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', 2000)\n",
    "import re\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import llm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from utils import *\n",
    "from utils_persona import *\n",
    "import ast\n",
    "DATA_DIR = '/weka/scratch/djiang21/jo/psychometric-alignment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4abd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/eedi/'\n",
    "# DATA_PATH = 'data/wordbank/'\n",
    "# DATA_PATH = 'data/duolingo/'\n",
    "df = pd.read_csv(DATA_DIR + DATA_PATH + 'all_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2b816",
   "metadata": {},
   "source": [
    "# prepare train val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50fb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(DATA_DIR + DATA_PATH + 'test_data.csv')\n",
    "test_user_ids = list(results['UserId'].unique())\n",
    "test_question_ids = list(results['QuestionId'].unique())\n",
    "\n",
    "print('before dropping test users:', len(df))\n",
    "print('UserId', df['UserId'].nunique())\n",
    "df = df[~df['UserId'].isin(test_user_ids)]\n",
    "print('after dropping test users:', len(df))\n",
    "print('UserId', df['UserId'].nunique())\n",
    "\n",
    "print('before dropping test q:', len(df))\n",
    "print('QuestionId', df['QuestionId'].nunique())\n",
    "df = df[~df['QuestionId'].isin(test_question_ids)]\n",
    "print('after dropping test q:', len(df))\n",
    "print('QuestionId', df['QuestionId'].nunique())\n",
    "\n",
    "df = df.sort_values(['UserId', 'DateAnswered'])\n",
    "\n",
    "if 'countries' in df:\n",
    "    df['countries_full'] = df['countries'].apply(lambda x: map_country_codes(x))\n",
    "\n",
    "if 'duolingo' in DATA_PATH:\n",
    "    df['persona'] = df.apply(create_persona_duolingo_first_pov, axis=1)\n",
    "    input_response_column = 'Correctness'\n",
    "elif 'wordbank' in DATA_PATH:\n",
    "    df['persona'] = df.apply(create_persona_wordbank_first_pov, axis=1)\n",
    "    input_response_column = 'Correctness'\n",
    "else:\n",
    "    df['persona'] = df.apply(create_persona_3basic, axis=1)\n",
    "    input_response_column = 'answer_choice'\n",
    "\n",
    "if 'AnswerValue' in df.columns and 'CorrectAnswer' in df.columns:\n",
    "    df['answer_choice'] = df['AnswerValue'].apply(lambda x: number_to_letter(x))\n",
    "    df['correct_answer_choice'] = df['CorrectAnswer'].apply(lambda x: number_to_letter(x))\n",
    "df['Correctness'] = df['IsCorrect'].apply(lambda x: map_binary_to_correctness(x))\n",
    "\n",
    "random.seed(0)\n",
    "all_ids = list(df['UserId'].unique())\n",
    "val_df_ids = random.sample(all_ids, int(0.1*len(all_ids)))\n",
    "train_df_ids = list(set(all_ids) - set(val_df_ids))\n",
    "train_df = df[df['UserId'].isin(train_df_ids)]\n",
    "val_df = df[df['UserId'].isin(val_df_ids)]\n",
    "print(len(train_df))\n",
    "print(train_df['UserId'].nunique())\n",
    "print(len(val_df))\n",
    "print(val_df['UserId'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea593997",
   "metadata": {},
   "source": [
    "# take subsets of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ids(train_df_ids):\n",
    "    random.seed(0)\n",
    "    current_ids = train_df_ids[:]\n",
    "    samples = [train_df_ids]\n",
    "    \n",
    "    for _ in range(8):\n",
    "        current_ids = random.sample(current_ids, int(0.5*len(current_ids)))\n",
    "        samples.append(current_ids)\n",
    "    \n",
    "    return samples\n",
    "train_samples = sample_ids(train_df_ids)\n",
    "for t in train_samples:\n",
    "    print(len(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11337d58",
   "metadata": {},
   "source": [
    "# save answer_id files for val and each subset of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_samples[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '20240612'\n",
    "\n",
    "#================eedi=================\n",
    "# MIN_NUM = 3\n",
    "# MAX_NUM = 10\n",
    "# NUM_REPEAT_VAL = 1\n",
    "# NUM_REPEAT_TRAIN = 20\n",
    "# define_sequence = ['UserId', 'QuizId']\n",
    "\n",
    "#================wordbank=================\n",
    "# MIN_NUM = 2\n",
    "# MAX_NUM = 49\n",
    "# NUM_REPEAT_VAL = 10\n",
    "# NUM_REPEAT_TRAIN = 100\n",
    "# define_sequence = ['UserId']\n",
    "\n",
    "#================duolingo=================\n",
    "# MIN_NUM = 2\n",
    "# MAX_NUM = 49\n",
    "# NUM_REPEAT_VAL = 10\n",
    "# NUM_REPEAT_TRAIN = 200\n",
    "# define_sequence = ['UserId']\n",
    "\n",
    "def generate_random_data_points(df, num_iter, num_example, min_num_example, define_sequence=['UserId', 'QuizId']):\n",
    "    print('min_num_example', min_num_example + 1)\n",
    "    print('max_num_example', num_example + 1)\n",
    "    print(datetime.now())\n",
    "    instruction_list = []\n",
    "    input_list = []\n",
    "    output_list = []\n",
    "    input_answer_id_list = []\n",
    "    output_answer_id_list = []\n",
    "    is_correct_list = []\n",
    "    for iteration in range(num_iter):\n",
    "        for _, group_df in df.groupby(define_sequence):\n",
    "            total_problems = len(group_df)\n",
    "            if total_problems > num_example:\n",
    "                input_df = group_df.sample(n=num_example)\n",
    "                output_df = group_df.loc[~group_df.index.isin(input_df.index)]\n",
    "                output_df = output_df.sample(n=1)\n",
    "            elif total_problems > min_num_example and total_problems <= num_example:\n",
    "                input_df = group_df.sample(n=total_problems - 1)\n",
    "                output_df = group_df.loc[~group_df.index.isin(input_df.index)]\n",
    "            else:\n",
    "                continue\n",
    "            output_answer_id = output_df['AnswerId'].iloc[0]\n",
    "            input_answer_id_list.append(list(input_df['AnswerId']))\n",
    "            output_answer_id_list.append(output_answer_id)\n",
    "            is_correct_list_sample = list(input_df['IsCorrect'])\n",
    "            is_correct_list_sample.append(output_df['IsCorrect'].iloc[0])\n",
    "            is_correct_list.append(is_correct_list_sample)\n",
    "    return input_answer_id_list, output_answer_id_list, is_correct_list\n",
    "\n",
    "val_data = pd.DataFrame()\n",
    "input_answer_id_list, output_answer_id_list, is_correct_list = generate_random_data_points(val_df, NUM_REPEAT_VAL, MAX_NUM, MIN_NUM, define_sequence=define_sequence)\n",
    "val_data['input_answer_id'] = input_answer_id_list\n",
    "val_data['output_answer_id'] = output_answer_id_list\n",
    "val_data['is_correct_list'] = is_correct_list\n",
    "val_data = val_data.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "print('len(val_data) after', len(val_data))\n",
    "val_data.to_csv(DATA_DIR + DATA_PATH + 'val_answer_id_' + DATE + '_' + str(val_df['UserId'].nunique()) + '.csv', index=False)\n",
    "\n",
    "for s in train_samples:\n",
    "    train_df = df[df['UserId'].isin(s)]\n",
    "    print('len(train_df)', len(train_df))\n",
    "    print('unique users', train_df['UserId'].nunique())\n",
    "    train_data = pd.DataFrame()\n",
    "    input_answer_id_list, output_answer_id_list, is_correct_list = generate_random_data_points(train_df, NUM_REPEAT_TRAIN, MAX_NUM, MIN_NUM, define_sequence=define_sequence)\n",
    "    train_data['input_answer_id'] = input_answer_id_list\n",
    "    train_data['output_answer_id'] = output_answer_id_list\n",
    "    train_data['is_correct_list'] = is_correct_list\n",
    "    train_data = train_data.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    print('len(train_data) after', len(train_data))   \n",
    "    train_data.to_csv(DATA_DIR + DATA_PATH + 'train_answer_id_' + DATE + '_' + str(train_df['UserId'].nunique()) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2722b8",
   "metadata": {},
   "source": [
    "# save jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de805f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = DATA_DIR + DATA_PATH\n",
    "PERSONA_TEMPLATE = 'first_pov'\n",
    "\n",
    "def get_answer_id_list(x):\n",
    "    input_id = x[0]\n",
    "    output_id = x[1]\n",
    "    result = ast.literal_eval(input_id)\n",
    "    result.append(output_id)\n",
    "    return result \n",
    "\n",
    "def df_to_jsonl(df, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for _, row in df.iterrows():\n",
    "            jsonl_obj = {\"instruction\": row[\"instruction\"], \"input\": row[\"input\"]}\n",
    "            file.write(json.dumps(jsonl_obj) + '\\n')\n",
    "\n",
    "def save_data_jsonl(answer_id_df, sample_df, input_response_column, tmp_data_path):\n",
    "    instruction_list = []\n",
    "    input_list = []\n",
    "    answer_id_list = []\n",
    "    if PERSONA_TEMPLATE == 'first_pov':\n",
    "        student_answer_indicator = 'Your answer'\n",
    "    elif PERSONA_TEMPLATE == 'third_pov':\n",
    "        student_answer_indicator = 'Student answer'\n",
    "    for i in range(len(answer_id_df)):\n",
    "        order_list = answer_id_df.iloc[i]['answer_id_list']\n",
    "        # Create a dictionary to map AnswerId to position\n",
    "        order_dict = {id: index for index, id in enumerate(order_list)}\n",
    "\n",
    "        # Create a new column for sorting\n",
    "        if 'sort_order' in sample_df:\n",
    "            sample_df = sample_df.drop(['sort_order'], axis=1)\n",
    "        sample_df['sort_order'] = sample_df['AnswerId'].map(order_dict)\n",
    "        input_df = sample_df.dropna(subset=['sort_order']).sort_values('sort_order').drop('sort_order', axis=1).copy()\n",
    "        if len(input_df) > 0:\n",
    "            if input_response_column == 'Correctness':\n",
    "                concatenated_string = 'Question:\\n' + '\\nQuestion:\\n'.join(input_df['problem'] + '\\n' + student_answer_indicator + ':\\n' + input_df[input_response_column])\n",
    "            else:\n",
    "                concatenated_string = 'Question:\\n' + '\\nQuestion:\\n'.join(input_df['problem'] + '\\n' + student_answer_indicator + ':\\n' + input_df[input_response_column] + '\\nTrue answer:\\n' + input_df['correct_answer_choice'])\n",
    "            instruction_list.append(input_df['persona'].iloc[0])\n",
    "            input_list.append(concatenated_string)\n",
    "            answer_id_list.append(list(input_df['AnswerId']))\n",
    "\n",
    "    json_data = pd.DataFrame()\n",
    "    json_data['instruction'] = instruction_list\n",
    "    json_data['input'] = input_list\n",
    "    json_file_path = tmp_data_path + '.jsonl'\n",
    "    df_to_jsonl(json_data, json_file_path)\n",
    "    return json_file_path\n",
    "\n",
    "all_answer_id_files = [f for f in os.listdir(TRAIN_DATA_DIR) if 'answer_id' in f and 'csv' in f and DATE in f]\n",
    "all_answer_id_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in all_answer_id_files:\n",
    "    print(datetime.now())\n",
    "    answer_id_file = TRAIN_DATA_DIR + f\n",
    "    answer_id_df = pd.read_csv(answer_id_file)\n",
    "    answer_id_df['answer_id_list'] = answer_id_df[['input_answer_id', 'output_answer_id']].apply(lambda x: get_answer_id_list(x), axis=1)\n",
    "    tmp_data_path = TRAIN_DATA_DIR + DATE + '_data_with_' + answer_id_file.split('/')[-1].split('.csv')[0]\n",
    "    json_file_path = save_data_jsonl(answer_id_df, df, input_response_column, tmp_data_path)\n",
    "    print(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('json', data_files='/weka/scratch/djiang21/jo/psychometric-alignment/data/eedi/20240612_data_with_val_answer_id_20240612_213.jsonl', split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1dfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(row):\n",
    "    return (\"{instruction}\\n{input}\").format_map(row)\n",
    "\n",
    "print(formatting_func(data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
